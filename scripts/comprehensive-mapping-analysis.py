#!/usr/bin/env python3
"""
Script phÃ¢n tÃ­ch toÃ n diá»‡n Ä‘á»ƒ mapping hÃ¬nh áº£nh Cloudinary vá»›i sáº£n pháº©m
Má»¥c tiÃªu: TÄƒng tá»· lá»‡ cÃ³ hÃ¬nh tá»« 36.1% lÃªn 60%+
"""

import json
import urllib.request
import urllib.parse
import re
import difflib
from datetime import datetime
from pathlib import Path

# Supabase configuration
SUPABASE_URL = 'https://zgrfqkytbmahxcbgpkxx.supabase.co'
SUPABASE_ANON_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InpncmZxa3l0Ym1haHhjYmdwa3h4Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDYxNjI1MTAsImV4cCI6MjA2MTczODUxMH0.a6giZZFMrj6jBhLip3ShOFCyTHt5dbe31UDGCECh0Zs'

def supabase_request(method, endpoint, data=None):
    """Thá»±c hiá»‡n request Ä‘áº¿n Supabase API"""
    url = f"{SUPABASE_URL}/rest/v1/{endpoint}"
    
    headers = {
        'apikey': SUPABASE_ANON_KEY,
        'Authorization': f'Bearer {SUPABASE_ANON_KEY}',
        'Content-Type': 'application/json'
    }
    
    try:
        if data:
            data_bytes = json.dumps(data).encode('utf-8')
            req = urllib.request.Request(url, data=data_bytes, headers=headers, method=method)
        else:
            req = urllib.request.Request(url, headers=headers, method=method)
        
        with urllib.request.urlopen(req) as response:
            return response.getcode(), response.read().decode('utf-8')
            
    except urllib.error.HTTPError as e:
        return e.code, e.read().decode('utf-8')
    except Exception as e:
        return 0, str(e)

def get_products_without_images():
    """Láº¥y danh sÃ¡ch sáº£n pháº©m chÆ°a cÃ³ hÃ¬nh áº£nh"""
    print("ğŸ“¥ Äang láº¥y danh sÃ¡ch sáº£n pháº©m chÆ°a cÃ³ hÃ¬nh áº£nh...")
    
    status_code, response = supabase_request('GET', 'fabrics?select=id,code,name,image&or=(image.is.null,image.eq.)')
    
    if status_code == 200:
        try:
            products = json.loads(response)
            print(f"âœ… TÃ¬m tháº¥y {len(products)} sáº£n pháº©m chÆ°a cÃ³ hÃ¬nh áº£nh")
            return products
        except Exception as e:
            print(f"âŒ Lá»—i parse response: {e}")
            return []
    else:
        print(f"âŒ Lá»—i láº¥y dá»¯ liá»‡u: {status_code} - {response}")
        return []

def load_cloudinary_analysis():
    """Load dá»¯ liá»‡u phÃ¢n tÃ­ch Cloudinary"""
    try:
        with open('cloudinary-audit-analysis.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ… ÄÃ£ load dá»¯ liá»‡u Cloudinary: {data['summary']['total_images']} hÃ¬nh áº£nh")
        return data
        
    except Exception as e:
        print(f"âŒ Lá»—i load dá»¯ liá»‡u Cloudinary: {e}")
        return None

def normalize_code_for_comparison(code):
    """Chuáº©n hÃ³a mÃ£ Ä‘á»ƒ so sÃ¡nh"""
    if not code:
        return ""
    
    # Chuyá»ƒn vá» uppercase
    normalized = str(code).upper().strip()
    
    # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t
    normalized = re.sub(r'[^\w\-]', '', normalized)
    
    # Loáº¡i bá» prefix fabrics/
    if normalized.startswith('FABRICS'):
        normalized = normalized[7:]
    
    return normalized

def extract_fabric_code_from_cloudinary_name(name):
    """TrÃ­ch xuáº¥t mÃ£ váº£i tá»« tÃªn file Cloudinary"""
    # Loáº¡i bá» extension
    name = re.sub(r'\.(jpg|jpeg|png|gif|webp)$', '', name, flags=re.IGNORECASE)
    
    # Loáº¡i bá» prefix fabrics/
    name = re.sub(r'^fabrics/', '', name, flags=re.IGNORECASE)
    
    # Loáº¡i bá» suffix _edited vÃ  cÃ¡c suffix khÃ¡c
    name = re.sub(r'_edited.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_copy.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_v\d+.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_[a-z0-9]{6,}$', '', name, flags=re.IGNORECASE)  # Random suffix
    
    return name.strip()

def calculate_similarity_score(product_code, image_name):
    """TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng giá»¯a mÃ£ sáº£n pháº©m vÃ  tÃªn hÃ¬nh áº£nh"""
    product_normalized = normalize_code_for_comparison(product_code)
    image_normalized = normalize_code_for_comparison(extract_fabric_code_from_cloudinary_name(image_name))
    
    if not product_normalized or not image_normalized:
        return 0
    
    # Exact match
    if product_normalized == image_normalized:
        return 1.0
    
    # Substring match
    if product_normalized in image_normalized or image_normalized in product_normalized:
        return 0.9
    
    # Sequence similarity
    similarity = difflib.SequenceMatcher(None, product_normalized, image_normalized).ratio()
    
    return similarity

def find_matching_images_for_product(product, cloudinary_data):
    """TÃ¬m hÃ¬nh áº£nh phÃ¹ há»£p cho sáº£n pháº©m"""
    product_code = product['code']
    matches = []
    
    # Láº¥y táº¥t cáº£ hÃ¬nh áº£nh tá»« analysis
    all_images = []
    
    # ThÃªm exact matches
    for match in cloudinary_data['analysis']['exact_matches']:
        all_images.append({
            'type': 'exact_match',
            'image': match['image'],
            'confidence': 1.0,
            'priority': 1
        })
    
    # ThÃªm similar matches
    for match in cloudinary_data['analysis']['similar_matches']:
        all_images.append({
            'type': 'similar_match',
            'image': match['image'],
            'confidence': 0.8,
            'priority': 2
        })
    
    # ThÃªm edited images
    for img in cloudinary_data['analysis']['edited_images']:
        all_images.append({
            'type': 'edited_image',
            'image': {
                'public_id': img['public_id'],
                'display_name': img['display_name'],
                'url': img['url'],
                'extracted_code': img['extracted_code']
            },
            'confidence': 0.85,
            'priority': 1  # Æ¯u tiÃªn cao cho edited
        })
    
    # ThÃªm unmapped images
    for img in cloudinary_data['analysis']['unmapped_images']:
        all_images.append({
            'type': 'unmapped_image',
            'image': {
                'public_id': img['public_id'],
                'display_name': img['display_name'],
                'url': img['url'],
                'extracted_code': img['extracted_code']
            },
            'confidence': 0.5,
            'priority': 3
        })
    
    # TÃ­nh similarity cho tá»«ng hÃ¬nh áº£nh
    for img_data in all_images:
        img = img_data['image']
        
        # TÃ­nh similarity vá»›i display_name
        similarity_display = calculate_similarity_score(product_code, img['display_name'])
        
        # TÃ­nh similarity vá»›i extracted_code
        similarity_extracted = calculate_similarity_score(product_code, img.get('extracted_code', ''))
        
        # TÃ­nh similarity vá»›i public_id
        similarity_public_id = calculate_similarity_score(product_code, img['public_id'])
        
        # Láº¥y similarity cao nháº¥t
        max_similarity = max(similarity_display, similarity_extracted, similarity_public_id)
        
        # Äiá»u chá»‰nh confidence dá»±a trÃªn type
        if img_data['type'] == 'edited_image' and max_similarity >= 0.7:
            final_confidence = min(0.95, max_similarity + 0.1)  # Bonus cho edited
        elif img_data['type'] == 'exact_match':
            final_confidence = max_similarity
        else:
            final_confidence = max_similarity * img_data['confidence']
        
        # Chá»‰ thÃªm náº¿u similarity >= 0.6
        if max_similarity >= 0.6:
            matches.append({
                'image': img,
                'type': img_data['type'],
                'similarity': max_similarity,
                'confidence': final_confidence,
                'priority': img_data['priority'],
                'similarity_breakdown': {
                    'display_name': similarity_display,
                    'extracted_code': similarity_extracted,
                    'public_id': similarity_public_id
                }
            })
    
    # Sáº¯p xáº¿p theo priority, confidence, similarity
    matches.sort(key=lambda x: (-x['priority'], -x['confidence'], -x['similarity']))
    
    return matches[:10]  # Top 10 matches

def categorize_mapping_confidence(confidence):
    """PhÃ¢n loáº¡i Ä‘á»™ tin cáº­y mapping"""
    if confidence >= 0.9:
        return 'HIGH'
    elif confidence >= 0.75:
        return 'MEDIUM'
    elif confidence >= 0.6:
        return 'LOW'
    else:
        return 'VERY_LOW'

def analyze_all_products(products_without_images, cloudinary_data):
    """PhÃ¢n tÃ­ch táº¥t cáº£ sáº£n pháº©m chÆ°a cÃ³ hÃ¬nh"""
    print(f"ğŸ” Äang phÃ¢n tÃ­ch {len(products_without_images)} sáº£n pháº©m...")
    
    analysis_results = {
        'high_confidence': [],
        'medium_confidence': [],
        'low_confidence': [],
        'no_matches': [],
        'statistics': {
            'total_products': len(products_without_images),
            'with_matches': 0,
            'without_matches': 0,
            'high_confidence_count': 0,
            'medium_confidence_count': 0,
            'low_confidence_count': 0
        }
    }
    
    for i, product in enumerate(products_without_images):
        if (i + 1) % 50 == 0:
            print(f"   ğŸ“Š ÄÃ£ xá»­ lÃ½ {i + 1}/{len(products_without_images)} sáº£n pháº©m...")
        
        matches = find_matching_images_for_product(product, cloudinary_data)
        
        if matches:
            best_match = matches[0]
            confidence_level = categorize_mapping_confidence(best_match['confidence'])
            
            product_analysis = {
                'product': product,
                'best_match': best_match,
                'all_matches': matches,
                'confidence_level': confidence_level,
                'recommendation': 'auto' if confidence_level == 'HIGH' else 'manual'
            }
            
            if confidence_level == 'HIGH':
                analysis_results['high_confidence'].append(product_analysis)
                analysis_results['statistics']['high_confidence_count'] += 1
            elif confidence_level == 'MEDIUM':
                analysis_results['medium_confidence'].append(product_analysis)
                analysis_results['statistics']['medium_confidence_count'] += 1
            else:
                analysis_results['low_confidence'].append(product_analysis)
                analysis_results['statistics']['low_confidence_count'] += 1
            
            analysis_results['statistics']['with_matches'] += 1
        else:
            analysis_results['no_matches'].append(product)
            analysis_results['statistics']['without_matches'] += 1
    
    return analysis_results

def create_comprehensive_report(analysis_results):
    """Táº¡o bÃ¡o cÃ¡o toÃ n diá»‡n"""
    stats = analysis_results['statistics']
    
    # TÃ­nh toÃ¡n potential coverage
    potential_mappings = stats['high_confidence_count'] + stats['medium_confidence_count']
    current_with_images = 120  # Hiá»‡n táº¡i
    total_products = 332
    
    current_coverage = (current_with_images / total_products) * 100
    potential_coverage = ((current_with_images + potential_mappings) / total_products) * 100
    
    report_content = f"""# ğŸ“Š BÃO CÃO PHÃ‚N TÃCH MAPPING TOÃ€N DIá»†N

## ğŸ“ˆ Tá»•ng quan:
- **Sáº£n pháº©m chÆ°a cÃ³ hÃ¬nh:** {stats['total_products']}
- **TÃ¬m tháº¥y matches:** {stats['with_matches']} ({stats['with_matches']/stats['total_products']*100:.1f}%)
- **KhÃ´ng cÃ³ matches:** {stats['without_matches']} ({stats['without_matches']/stats['total_products']*100:.1f}%)

## ğŸ¯ PhÃ¢n loáº¡i theo Ä‘á»™ tin cáº­y:
- **HIGH (â‰¥90%):** {stats['high_confidence_count']} sáº£n pháº©m - **Tá»° Äá»˜NG MAPPING**
- **MEDIUM (75-89%):** {stats['medium_confidence_count']} sáº£n pháº©m - **XEM XÃ‰T THá»¦ CÃ”NG**
- **LOW (60-74%):** {stats['low_confidence_count']} sáº£n pháº©m - **Cáº¦N KIá»‚M TRA Ká»¸**

## ğŸ“Š Dá»± bÃ¡o coverage:
- **Hiá»‡n táº¡i:** {current_coverage:.1f}% ({current_with_images}/{total_products})
- **Sau HIGH mapping:** {((current_with_images + stats['high_confidence_count'])/total_products)*100:.1f}% (+{stats['high_confidence_count']})
- **Sau MEDIUM mapping:** {potential_coverage:.1f}% (+{potential_mappings})
- **Má»¥c tiÃªu 60%:** {'âœ… Äáº T ÄÆ¯á»¢C' if potential_coverage >= 60 else 'âŒ Cáº¦N THÃŠM'}

## ğŸ¯ HIGH CONFIDENCE MAPPINGS ({stats['high_confidence_count']})
**Khuyáº¿n nghá»‹: Tá»° Äá»˜NG MAPPING**

"""
    
    for i, item in enumerate(analysis_results['high_confidence'][:20], 1):
        product = item['product']
        match = item['best_match']
        
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
        report_content += f"   ğŸ–¼ï¸ {match['image']['display_name']}\n"
        report_content += f"   ğŸ“Š Confidence: {match['confidence']:.1%} | Type: {match['type']}\n"
        report_content += f"   ğŸ”— {match['image']['url']}\n\n"
    
    if len(analysis_results['high_confidence']) > 20:
        report_content += f"... vÃ  {len(analysis_results['high_confidence']) - 20} sáº£n pháº©m khÃ¡c\n\n"
    
    # Medium confidence
    report_content += f"""## ğŸ” MEDIUM CONFIDENCE MAPPINGS ({stats['medium_confidence_count']})
**Khuyáº¿n nghá»‹: XEM XÃ‰T THá»¦ CÃ”NG**

"""
    
    for i, item in enumerate(analysis_results['medium_confidence'][:15], 1):
        product = item['product']
        match = item['best_match']
        
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
        report_content += f"   ğŸ–¼ï¸ {match['image']['display_name']}\n"
        report_content += f"   ğŸ“Š Confidence: {match['confidence']:.1%} | Type: {match['type']}\n"
        report_content += f"   ğŸ”— {match['image']['url']}\n\n"
    
    if len(analysis_results['medium_confidence']) > 15:
        report_content += f"... vÃ  {len(analysis_results['medium_confidence']) - 15} sáº£n pháº©m khÃ¡c\n\n"
    
    # Low confidence
    report_content += f"""## âš ï¸ LOW CONFIDENCE MAPPINGS ({stats['low_confidence_count']})
**Khuyáº¿n nghá»‹: Cáº¦N KIá»‚M TRA Ká»¸**

"""
    
    for i, item in enumerate(analysis_results['low_confidence'][:10], 1):
        product = item['product']
        match = item['best_match']
        
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
        report_content += f"   ğŸ–¼ï¸ {match['image']['display_name']}\n"
        report_content += f"   ğŸ“Š Confidence: {match['confidence']:.1%} | Type: {match['type']}\n"
        report_content += f"   ğŸ”— {match['image']['url']}\n\n"
    
    # No matches
    report_content += f"""## âŒ KHÃ”NG TÃŒM THáº¤Y MATCHES ({stats['without_matches']})
**Khuyáº¿n nghá»‹: Cáº¦N UPLOAD HÃŒNH Má»šI**

"""
    
    for i, product in enumerate(analysis_results['no_matches'][:15], 1):
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
    
    if len(analysis_results['no_matches']) > 15:
        report_content += f"... vÃ  {len(analysis_results['no_matches']) - 15} sáº£n pháº©m khÃ¡c\n"
    
    report_content += f"""

## ğŸ“‹ Káº¾ HOáº CH THá»°C HIá»†N

### ğŸ¯ BÆ°á»›c 1: Auto Mapping (HIGH Confidence)
- **Sá»‘ lÆ°á»£ng:** {stats['high_confidence_count']} sáº£n pháº©m
- **PhÆ°Æ¡ng phÃ¡p:** Tá»± Ä‘á»™ng mapping qua script
- **Káº¿t quáº£ dá»± kiáº¿n:** TÄƒng coverage lÃªn {((current_with_images + stats['high_confidence_count'])/total_products)*100:.1f}%

### ğŸ” BÆ°á»›c 2: Manual Review (MEDIUM Confidence)
- **Sá»‘ lÆ°á»£ng:** {stats['medium_confidence_count']} sáº£n pháº©m
- **PhÆ°Æ¡ng phÃ¡p:** Xem xÃ©t thá»§ cÃ´ng tá»«ng trÆ°á»ng há»£p
- **Káº¿t quáº£ dá»± kiáº¿n:** TÄƒng coverage lÃªn {potential_coverage:.1f}%

### âš ï¸ BÆ°á»›c 3: Detailed Check (LOW Confidence)
- **Sá»‘ lÆ°á»£ng:** {stats['low_confidence_count']} sáº£n pháº©m
- **PhÆ°Æ¡ng phÃ¡p:** Kiá»ƒm tra ká»¹ lÆ°á»¡ng, cÃ³ thá»ƒ cáº§n chá»‰nh sá»­a
- **Káº¿t quáº£ dá»± kiáº¿n:** ThÃªm 20-50% sá»‘ sáº£n pháº©m nÃ y

### ğŸ“· BÆ°á»›c 4: Upload New Images
- **Sá»‘ lÆ°á»£ng:** {stats['without_matches']} sáº£n pháº©m
- **PhÆ°Æ¡ng phÃ¡p:** Chá»¥p/upload hÃ¬nh áº£nh má»›i
- **Æ¯u tiÃªn:** Sáº£n pháº©m cÃ³ sá»‘ lÆ°á»£ng lá»›n, vá»‹ trÃ­ dá»… tiáº¿p cáº­n

## ğŸ¯ Má»¥c tiÃªu 60% Coverage:
- **Cáº§n mapping thÃªm:** {max(0, int(total_products * 0.6) - current_with_images)} sáº£n pháº©m
- **CÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c:** {potential_mappings} sáº£n pháº©m (HIGH + MEDIUM)
- **Káº¿t luáº­n:** {'âœ… CÃ“ THá»‚ Äáº T Má»¤C TIÃŠU' if potential_mappings >= (total_products * 0.6 - current_with_images) else 'âŒ Cáº¦N THÃŠM HÃŒNH áº¢NH Má»šI'}

---
Táº¡o bá»Ÿi: comprehensive-mapping-analysis.py
Thá»i gian: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
"""
    
    return report_content

def save_analysis_data(analysis_results):
    """LÆ°u dá»¯ liá»‡u phÃ¢n tÃ­ch chi tiáº¿t"""
    with open('comprehensive-mapping-analysis.json', 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'analysis_results': analysis_results
        }, f, ensure_ascii=False, indent=2)
    
    print("ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u phÃ¢n tÃ­ch: comprehensive-mapping-analysis.json")

def main():
    print("ğŸš€ Báº®T Äáº¦U PHÃ‚N TÃCH MAPPING TOÃ€N DIá»†N")
    print("="*60)
    print("ğŸ¯ Má»¥c tiÃªu: TÄƒng coverage tá»« 36.1% lÃªn 60%+")
    
    # 1. Láº¥y sáº£n pháº©m chÆ°a cÃ³ hÃ¬nh
    products_without_images = get_products_without_images()
    if not products_without_images:
        print("âŒ KhÃ´ng cÃ³ sáº£n pháº©m nÃ o cáº§n mapping")
        return
    
    # 2. Load dá»¯ liá»‡u Cloudinary
    cloudinary_data = load_cloudinary_analysis()
    if not cloudinary_data:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Cloudinary")
        return
    
    # 3. PhÃ¢n tÃ­ch táº¥t cáº£ sáº£n pháº©m
    analysis_results = analyze_all_products(products_without_images, cloudinary_data)
    
    # 4. Táº¡o bÃ¡o cÃ¡o
    report = create_comprehensive_report(analysis_results)
    
    with open('BAO_CAO_MAPPING_TOAN_DIEN.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    # 5. LÆ°u dá»¯ liá»‡u chi tiáº¿t
    save_analysis_data(analysis_results)
    
    # 6. TÃ³m táº¯t káº¿t quáº£
    stats = analysis_results['statistics']
    print(f"\nğŸ‰ HOÃ€N Táº¤T PHÃ‚N TÃCH!")
    print(f"ğŸ“Š Káº¿t quáº£:")
    print(f"   ğŸ¯ HIGH confidence: {stats['high_confidence_count']} (auto mapping)")
    print(f"   ğŸ” MEDIUM confidence: {stats['medium_confidence_count']} (manual review)")
    print(f"   âš ï¸  LOW confidence: {stats['low_confidence_count']} (detailed check)")
    print(f"   âŒ No matches: {stats['without_matches']} (need new images)")
    
    potential_new = stats['high_confidence_count'] + stats['medium_confidence_count']
    new_coverage = ((120 + potential_new) / 332) * 100
    
    print(f"\nğŸ¯ Dá»± bÃ¡o coverage:")
    print(f"   ğŸ“ˆ Hiá»‡n táº¡i: 36.1% (120/332)")
    print(f"   ğŸ“ˆ Sau mapping: {new_coverage:.1f}% (+{potential_new})")
    print(f"   ğŸ¯ Má»¥c tiÃªu 60%: {'âœ… Äáº T ÄÆ¯á»¢C' if new_coverage >= 60 else 'âŒ Cáº¦N THÃŠM'}")
    
    print(f"\nğŸ“ Files Ä‘Ã£ táº¡o:")
    print(f"   ğŸ“‹ BAO_CAO_MAPPING_TOAN_DIEN.md")
    print(f"   ğŸ’¾ comprehensive-mapping-analysis.json")

if __name__ == "__main__":
    main()
