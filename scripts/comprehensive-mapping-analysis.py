#!/usr/bin/env python3
"""
Script ph√¢n t√≠ch to√†n di·ªán ƒë·ªÉ mapping h√¨nh ·∫£nh Cloudinary v·ªõi s·∫£n ph·∫©m
M·ª•c ti√™u: TƒÉng t·ª∑ l·ªá c√≥ h√¨nh t·ª´ 36.1% l√™n 60%+
"""

import json
import urllib.request
import urllib.parse
import re
import difflib
from datetime import datetime
from pathlib import Path

# Supabase configuration
SUPABASE_URL = 'https://zgrfqkytbmahxcbgpkxx.supabase.co'
SUPABASE_ANON_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InpncmZxa3l0Ym1haHhjYmdwa3h4Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDYxNjI1MTAsImV4cCI6MjA2MTczODUxMH0.a6giZZFMrj6jBhLip3ShOFCyTHt5dbe31UDGCECh0Zs'

def supabase_request(method, endpoint, data=None):
    """Th·ª±c hi·ªán request ƒë·∫øn Supabase API"""
    url = f"{SUPABASE_URL}/rest/v1/{endpoint}"
    
    headers = {
        'apikey': SUPABASE_ANON_KEY,
        'Authorization': f'Bearer {SUPABASE_ANON_KEY}',
        'Content-Type': 'application/json'
    }
    
    try:
        if data:
            data_bytes = json.dumps(data).encode('utf-8')
            req = urllib.request.Request(url, data=data_bytes, headers=headers, method=method)
        else:
            req = urllib.request.Request(url, headers=headers, method=method)
        
        with urllib.request.urlopen(req) as response:
            return response.getcode(), response.read().decode('utf-8')
            
    except urllib.error.HTTPError as e:
        return e.code, e.read().decode('utf-8')
    except Exception as e:
        return 0, str(e)

def get_products_without_images():
    """L·∫•y danh s√°ch s·∫£n ph·∫©m ch∆∞a c√≥ h√¨nh ·∫£nh"""
    print("üì• ƒêang l·∫•y danh s√°ch s·∫£n ph·∫©m ch∆∞a c√≥ h√¨nh ·∫£nh...")
    
    status_code, response = supabase_request('GET', 'fabrics?select=id,code,name,image&or=(image.is.null,image.eq.)')
    
    if status_code == 200:
        try:
            products = json.loads(response)
            print(f"‚úÖ T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m ch∆∞a c√≥ h√¨nh ·∫£nh")
            return products
        except Exception as e:
            print(f"‚ùå L·ªói parse response: {e}")
            return []
    else:
        print(f"‚ùå L·ªói l·∫•y d·ªØ li·ªáu: {status_code} - {response}")
        return []

def load_cloudinary_analysis():
    """Load d·ªØ li·ªáu ph√¢n t√≠ch Cloudinary"""
    try:
        with open('cloudinary-audit-analysis.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"‚úÖ ƒê√£ load d·ªØ li·ªáu Cloudinary: {data['summary']['total_images']} h√¨nh ·∫£nh")
        return data
        
    except Exception as e:
        print(f"‚ùå L·ªói load d·ªØ li·ªáu Cloudinary: {e}")
        return None

def normalize_code_for_comparison(code):
    """Chu·∫©n h√≥a m√£ ƒë·ªÉ so s√°nh"""
    if not code:
        return ""
    
    # Chuy·ªÉn v·ªÅ uppercase
    normalized = str(code).upper().strip()
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
    normalized = re.sub(r'[^\w\-]', '', normalized)
    
    # Lo·∫°i b·ªè prefix fabrics/
    if normalized.startswith('FABRICS'):
        normalized = normalized[7:]
    
    return normalized

def extract_fabric_code_from_cloudinary_name(name):
    """Tr√≠ch xu·∫•t m√£ v·∫£i t·ª´ t√™n file Cloudinary"""
    # Lo·∫°i b·ªè extension
    name = re.sub(r'\.(jpg|jpeg|png|gif|webp)$', '', name, flags=re.IGNORECASE)
    
    # Lo·∫°i b·ªè prefix fabrics/
    name = re.sub(r'^fabrics/', '', name, flags=re.IGNORECASE)
    
    # Lo·∫°i b·ªè suffix _edited v√† c√°c suffix kh√°c
    name = re.sub(r'_edited.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_copy.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_v\d+.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_[a-z0-9]{6,}$', '', name, flags=re.IGNORECASE)  # Random suffix
    
    return name.strip()

def calculate_similarity_score(product_code, image_name):
    """T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng gi·ªØa m√£ s·∫£n ph·∫©m v√† t√™n h√¨nh ·∫£nh"""
    product_normalized = normalize_code_for_comparison(product_code)
    image_normalized = normalize_code_for_comparison(extract_fabric_code_from_cloudinary_name(image_name))
    
    if not product_normalized or not image_normalized:
        return 0
    
    # Exact match
    if product_normalized == image_normalized:
        return 1.0
    
    # Substring match
    if product_normalized in image_normalized or image_normalized in product_normalized:
        return 0.9
    
    # Sequence similarity
    similarity = difflib.SequenceMatcher(None, product_normalized, image_normalized).ratio()
    
    return similarity

def find_matching_images_for_product(product, cloudinary_data):
    """T√¨m h√¨nh ·∫£nh ph√π h·ª£p cho s·∫£n ph·∫©m"""
    product_code = product['code']
    matches = []
    
    # L·∫•y t·∫•t c·∫£ h√¨nh ·∫£nh t·ª´ analysis
    all_images = []
    
    # Th√™m exact matches
    for match in cloudinary_data['analysis']['exact_matches']:
        all_images.append({
            'type': 'exact_match',
            'image': match['image'],
            'confidence': 1.0,
            'priority': 1
        })
    
    # Th√™m similar matches
    for match in cloudinary_data['analysis']['similar_matches']:
        all_images.append({
            'type': 'similar_match',
            'image': match['image'],
            'confidence': 0.8,
            'priority': 2
        })
    
    # Th√™m edited images
    for img in cloudinary_data['analysis']['edited_images']:
        all_images.append({
            'type': 'edited_image',
            'image': {
                'public_id': img['public_id'],
                'display_name': img['display_name'],
                'url': img['url'],
                'extracted_code': img['extracted_code']
            },
            'confidence': 0.85,
            'priority': 1  # ∆Øu ti√™n cao cho edited
        })
    
    # Th√™m unmapped images
    for img in cloudinary_data['analysis']['unmapped_images']:
        all_images.append({
            'type': 'unmapped_image',
            'image': {
                'public_id': img['public_id'],
                'display_name': img['display_name'],
                'url': img['url'],
                'extracted_code': img['extracted_code']
            },
            'confidence': 0.5,
            'priority': 3
        })
    
    # T√≠nh similarity cho t·ª´ng h√¨nh ·∫£nh
    for img_data in all_images:
        img = img_data['image']
        
        # T√≠nh similarity v·ªõi display_name
        similarity_display = calculate_similarity_score(product_code, img['display_name'])
        
        # T√≠nh similarity v·ªõi extracted_code
        similarity_extracted = calculate_similarity_score(product_code, img.get('extracted_code', ''))
        
        # T√≠nh similarity v·ªõi public_id
        similarity_public_id = calculate_similarity_score(product_code, img['public_id'])
        
        # L·∫•y similarity cao nh·∫•t
        max_similarity = max(similarity_display, similarity_extracted, similarity_public_id)
        
        # ƒêi·ªÅu ch·ªânh confidence d·ª±a tr√™n type
        if img_data['type'] == 'edited_image' and max_similarity >= 0.7:
            final_confidence = min(0.95, max_similarity + 0.1)  # Bonus cho edited
        elif img_data['type'] == 'exact_match':
            final_confidence = max_similarity
        else:
            final_confidence = max_similarity * img_data['confidence']
        
        # Ch·ªâ th√™m n·∫øu similarity >= 0.6
        if max_similarity >= 0.6:
            matches.append({
                'image': img,
                'type': img_data['type'],
                'similarity': max_similarity,
                'confidence': final_confidence,
                'priority': img_data['priority'],
                'similarity_breakdown': {
                    'display_name': similarity_display,
                    'extracted_code': similarity_extracted,
                    'public_id': similarity_public_id
                }
            })
    
    # S·∫Øp x·∫øp theo priority, confidence, similarity
    matches.sort(key=lambda x: (-x['priority'], -x['confidence'], -x['similarity']))
    
    return matches[:10]  # Top 10 matches

def categorize_mapping_confidence(confidence):
    """Ph√¢n lo·∫°i ƒë·ªô tin c·∫≠y mapping"""
    if confidence >= 0.9:
        return 'HIGH'
    elif confidence >= 0.75:
        return 'MEDIUM'
    elif confidence >= 0.6:
        return 'LOW'
    else:
        return 'VERY_LOW'

def analyze_all_products(products_without_images, cloudinary_data):
    """Ph√¢n t√≠ch t·∫•t c·∫£ s·∫£n ph·∫©m ch∆∞a c√≥ h√¨nh"""
    print(f"üîç ƒêang ph√¢n t√≠ch {len(products_without_images)} s·∫£n ph·∫©m...")
    
    analysis_results = {
        'high_confidence': [],
        'medium_confidence': [],
        'low_confidence': [],
        'no_matches': [],
        'statistics': {
            'total_products': len(products_without_images),
            'with_matches': 0,
            'without_matches': 0,
            'high_confidence_count': 0,
            'medium_confidence_count': 0,
            'low_confidence_count': 0
        }
    }
    
    for i, product in enumerate(products_without_images):
        if (i + 1) % 50 == 0:
            print(f"   üìä ƒê√£ x·ª≠ l√Ω {i + 1}/{len(products_without_images)} s·∫£n ph·∫©m...")
        
        matches = find_matching_images_for_product(product, cloudinary_data)
        
        if matches:
            best_match = matches[0]
            confidence_level = categorize_mapping_confidence(best_match['confidence'])
            
            product_analysis = {
                'product': product,
                'best_match': best_match,
                'all_matches': matches,
                'confidence_level': confidence_level,
                'recommendation': 'auto' if confidence_level == 'HIGH' else 'manual'
            }
            
            if confidence_level == 'HIGH':
                analysis_results['high_confidence'].append(product_analysis)
                analysis_results['statistics']['high_confidence_count'] += 1
            elif confidence_level == 'MEDIUM':
                analysis_results['medium_confidence'].append(product_analysis)
                analysis_results['statistics']['medium_confidence_count'] += 1
            else:
                analysis_results['low_confidence'].append(product_analysis)
                analysis_results['statistics']['low_confidence_count'] += 1
            
            analysis_results['statistics']['with_matches'] += 1
        else:
            analysis_results['no_matches'].append(product)
            analysis_results['statistics']['without_matches'] += 1
    
    return analysis_results

def create_comprehensive_report(analysis_results):
    """T·∫°o b√°o c√°o to√†n di·ªán"""
    stats = analysis_results['statistics']
    
    # T√≠nh to√°n potential coverage
    potential_mappings = stats['high_confidence_count'] + stats['medium_confidence_count']
    current_with_images = 120  # Hi·ªán t·∫°i
    total_products = 332
    
    current_coverage = (current_with_images / total_products) * 100
    potential_coverage = ((current_with_images + potential_mappings) / total_products) * 100
    
    report_content = f"""# üìä B√ÅO C√ÅO PH√ÇN T√çCH MAPPING TO√ÄN DI·ªÜN

## üìà T·ªïng quan:
- **S·∫£n ph·∫©m ch∆∞a c√≥ h√¨nh:** {stats['total_products']}
- **T√¨m th·∫•y matches:** {stats['with_matches']} ({stats['with_matches']/stats['total_products']*100:.1f}%)
- **Kh√¥ng c√≥ matches:** {stats['without_matches']} ({stats['without_matches']/stats['total_products']*100:.1f}%)

## üéØ Ph√¢n lo·∫°i theo ƒë·ªô tin c·∫≠y:
- **HIGH (‚â•90%):** {stats['high_confidence_count']} s·∫£n ph·∫©m - **T·ª∞ ƒê·ªòNG MAPPING**
- **MEDIUM (75-89%):** {stats['medium_confidence_count']} s·∫£n ph·∫©m - **XEM X√âT TH·ª¶ C√îNG**
- **LOW (60-74%):** {stats['low_confidence_count']} s·∫£n ph·∫©m - **C·∫¶N KI·ªÇM TRA K·ª∏**

## üìä D·ª± b√°o coverage:
- **Hi·ªán t·∫°i:** {current_coverage:.1f}% ({current_with_images}/{total_products})
- **Sau HIGH mapping:** {((current_with_images + stats['high_confidence_count'])/total_products)*100:.1f}% (+{stats['high_confidence_count']})
- **Sau MEDIUM mapping:** {potential_coverage:.1f}% (+{potential_mappings})
- **M·ª•c ti√™u 60%:** {'‚úÖ ƒê·∫†T ƒê∆Ø·ª¢C' if potential_coverage >= 60 else '‚ùå C·∫¶N TH√äM'}

## üéØ HIGH CONFIDENCE MAPPINGS ({stats['high_confidence_count']})
**Khuy·∫øn ngh·ªã: T·ª∞ ƒê·ªòNG MAPPING**

"""
    
    for i, item in enumerate(analysis_results['high_confidence'][:20], 1):
        product = item['product']
        match = item['best_match']
        
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
        report_content += f"   üñºÔ∏è {match['image']['display_name']}\n"
        report_content += f"   üìä Confidence: {match['confidence']:.1%} | Type: {match['type']}\n"
        report_content += f"   üîó {match['image']['url']}\n\n"
    
    if len(analysis_results['high_confidence']) > 20:
        report_content += f"... v√† {len(analysis_results['high_confidence']) - 20} s·∫£n ph·∫©m kh√°c\n\n"
    
    # Medium confidence
    report_content += f"""## üîç MEDIUM CONFIDENCE MAPPINGS ({stats['medium_confidence_count']})
**Khuy·∫øn ngh·ªã: XEM X√âT TH·ª¶ C√îNG**

"""
    
    for i, item in enumerate(analysis_results['medium_confidence'][:15], 1):
        product = item['product']
        match = item['best_match']
        
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
        report_content += f"   üñºÔ∏è {match['image']['display_name']}\n"
        report_content += f"   üìä Confidence: {match['confidence']:.1%} | Type: {match['type']}\n"
        report_content += f"   üîó {match['image']['url']}\n\n"
    
    if len(analysis_results['medium_confidence']) > 15:
        report_content += f"... v√† {len(analysis_results['medium_confidence']) - 15} s·∫£n ph·∫©m kh√°c\n\n"
    
    # Low confidence
    report_content += f"""## ‚ö†Ô∏è LOW CONFIDENCE MAPPINGS ({stats['low_confidence_count']})
**Khuy·∫øn ngh·ªã: C·∫¶N KI·ªÇM TRA K·ª∏**

"""
    
    for i, item in enumerate(analysis_results['low_confidence'][:10], 1):
        product = item['product']
        match = item['best_match']
        
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
        report_content += f"   üñºÔ∏è {match['image']['display_name']}\n"
        report_content += f"   üìä Confidence: {match['confidence']:.1%} | Type: {match['type']}\n"
        report_content += f"   üîó {match['image']['url']}\n\n"
    
    # No matches
    report_content += f"""## ‚ùå KH√îNG T√åM TH·∫§Y MATCHES ({stats['without_matches']})
**Khuy·∫øn ngh·ªã: C·∫¶N UPLOAD H√åNH M·ªöI**

"""
    
    for i, product in enumerate(analysis_results['no_matches'][:15], 1):
        report_content += f"{i}. **{product['code']}** - {product['name'][:50]}...\n"
    
    if len(analysis_results['no_matches']) > 15:
        report_content += f"... v√† {len(analysis_results['no_matches']) - 15} s·∫£n ph·∫©m kh√°c\n"
    
    report_content += f"""

## üìã K·∫æ HO·∫†CH TH·ª∞C HI·ªÜN

### üéØ B∆∞·ªõc 1: Auto Mapping (HIGH Confidence)
- **S·ªë l∆∞·ª£ng:** {stats['high_confidence_count']} s·∫£n ph·∫©m
- **Ph∆∞∆°ng ph√°p:** T·ª± ƒë·ªông mapping qua script
- **K·∫øt qu·∫£ d·ª± ki·∫øn:** TƒÉng coverage l√™n {((current_with_images + stats['high_confidence_count'])/total_products)*100:.1f}%

### üîç B∆∞·ªõc 2: Manual Review (MEDIUM Confidence)
- **S·ªë l∆∞·ª£ng:** {stats['medium_confidence_count']} s·∫£n ph·∫©m
- **Ph∆∞∆°ng ph√°p:** Xem x√©t th·ªß c√¥ng t·ª´ng tr∆∞·ªùng h·ª£p
- **K·∫øt qu·∫£ d·ª± ki·∫øn:** TƒÉng coverage l√™n {potential_coverage:.1f}%

### ‚ö†Ô∏è B∆∞·ªõc 3: Detailed Check (LOW Confidence)
- **S·ªë l∆∞·ª£ng:** {stats['low_confidence_count']} s·∫£n ph·∫©m
- **Ph∆∞∆°ng ph√°p:** Ki·ªÉm tra k·ªπ l∆∞·ª°ng, c√≥ th·ªÉ c·∫ßn ch·ªânh s·ª≠a
- **K·∫øt qu·∫£ d·ª± ki·∫øn:** Th√™m 20-50% s·ªë s·∫£n ph·∫©m n√†y

### üì∑ B∆∞·ªõc 4: Upload New Images
- **S·ªë l∆∞·ª£ng:** {stats['without_matches']} s·∫£n ph·∫©m
- **Ph∆∞∆°ng ph√°p:** Ch·ª•p/upload h√¨nh ·∫£nh m·ªõi
- **∆Øu ti√™n:** S·∫£n ph·∫©m c√≥ s·ªë l∆∞·ª£ng l·ªõn, v·ªã tr√≠ d·ªÖ ti·∫øp c·∫≠n

## üéØ M·ª•c ti√™u 60% Coverage:
- **C·∫ßn mapping th√™m:** {max(0, int(total_products * 0.6) - current_with_images)} s·∫£n ph·∫©m
- **C√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c:** {potential_mappings} s·∫£n ph·∫©m (HIGH + MEDIUM)
- **K·∫øt lu·∫≠n:** {'‚úÖ C√ì TH·ªÇ ƒê·∫†T M·ª§C TI√äU' if potential_mappings >= (total_products * 0.6 - current_with_images) else '‚ùå C·∫¶N TH√äM H√åNH ·∫¢NH M·ªöI'}

---
T·∫°o b·ªüi: comprehensive-mapping-analysis.py
Th·ªùi gian: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
"""
    
    return report_content

def save_analysis_data(analysis_results):
    """L∆∞u d·ªØ li·ªáu ph√¢n t√≠ch chi ti·∫øt"""
    with open('comprehensive-mapping-analysis.json', 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'analysis_results': analysis_results
        }, f, ensure_ascii=False, indent=2)
    
    print("üíæ ƒê√£ l∆∞u d·ªØ li·ªáu ph√¢n t√≠ch: comprehensive-mapping-analysis.json")

def main():
    print("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH MAPPING TO√ÄN DI·ªÜN")
    print("="*60)
    print("üéØ M·ª•c ti√™u: TƒÉng coverage t·ª´ 36.1% l√™n 60%+")
    
    # 1. L·∫•y s·∫£n ph·∫©m ch∆∞a c√≥ h√¨nh
    products_without_images = get_products_without_images()
    if not products_without_images:
        print("‚ùå Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o c·∫ßn mapping")
        return
    
    # 2. Load d·ªØ li·ªáu Cloudinary
    cloudinary_data = load_cloudinary_analysis()
    if not cloudinary_data:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu Cloudinary")
        return
    
    # 3. Ph√¢n t√≠ch t·∫•t c·∫£ s·∫£n ph·∫©m
    analysis_results = analyze_all_products(products_without_images, cloudinary_data)
    
    # 4. T·∫°o b√°o c√°o
    report = create_comprehensive_report(analysis_results)
    
    with open('BAO_CAO_MAPPING_TOAN_DIEN.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    # 5. L∆∞u d·ªØ li·ªáu chi ti·∫øt
    save_analysis_data(analysis_results)
    
    # 6. T√≥m t·∫Øt k·∫øt qu·∫£
    stats = analysis_results['statistics']
    print(f"\nüéâ HO√ÄN T·∫§T PH√ÇN T√çCH!")
    print(f"üìä K·∫øt qu·∫£:")
    print(f"   üéØ HIGH confidence: {stats['high_confidence_count']} (auto mapping)")
    print(f"   üîç MEDIUM confidence: {stats['medium_confidence_count']} (manual review)")
    print(f"   ‚ö†Ô∏è  LOW confidence: {stats['low_confidence_count']} (detailed check)")
    print(f"   ‚ùå No matches: {stats['without_matches']} (need new images)")
    
    potential_new = stats['high_confidence_count'] + stats['medium_confidence_count']
    new_coverage = ((120 + potential_new) / 332) * 100
    
    print(f"\nüéØ D·ª± b√°o coverage:")
    print(f"   üìà Hi·ªán t·∫°i: 36.1% (120/332)")
    print(f"   üìà Sau mapping: {new_coverage:.1f}% (+{potential_new})")
    print(f"   üéØ M·ª•c ti√™u 60%: {'‚úÖ ƒê·∫†T ƒê∆Ø·ª¢C' if new_coverage >= 60 else '‚ùå C·∫¶N TH√äM'}")
    
    print(f"\nüìÅ Files ƒë√£ t·∫°o:")
    print(f"   üìã BAO_CAO_MAPPING_TOAN_DIEN.md")
    print(f"   üíæ comprehensive-mapping-analysis.json")

if __name__ == "__main__":
    main()
